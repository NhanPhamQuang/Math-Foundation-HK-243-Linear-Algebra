{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1u6yJGEI7r5ZwR/RSDx7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NhanPhamQuang/Math-Foundation-HK-243-Linear-Algebra/blob/main/Linear_Algebra_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "8VIh_jBXdvr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prove that the transpose of the transpose of a matrix is the matrix itself: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$."
      ],
      "metadata": {
        "id": "hOAZEdt0c2Q_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF3z0gqFczVX",
        "outputId": "667820cb-f0b2-460b-ba42-a1920c62c01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor A:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Transpose of A (A^T):\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "\n",
            "Transpose of A^T ((A^T)^T):\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Is (A^T)^T equal to A? True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a sample tensor\n",
        "A = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "# Compute the transpose of A\n",
        "A_T = A.t()\n",
        "\n",
        "# Compute the transpose of A_T\n",
        "A_T_T = A_T.t()\n",
        "\n",
        "# Verify that A_T_T equals A\n",
        "is_equal = torch.equal(A, A_T_T)\n",
        "\n",
        "print(\"Original tensor A:\")\n",
        "print(A)\n",
        "print(\"\\nTranspose of A (A^T):\")\n",
        "print(A_T)\n",
        "print(\"\\nTranspose of A^T ((A^T)^T):\")\n",
        "print(A_T_T)\n",
        "print(\"\\nIs (A^T)^T equal to A?\", is_equal)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that sum and transposition commute: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$."
      ],
      "metadata": {
        "id": "9y3zv1HFdQRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two sample tensors\n",
        "A = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "B = torch.tensor([[7, 8, 9],\n",
        "                  [10, 11, 12]])\n",
        "\n",
        "# Compute A^T + B^T\n",
        "A_T = A.t()  # Transpose for 2D tensor\n",
        "B_T = B.t()\n",
        "left_side = A_T + B_T\n",
        "\n",
        "# Compute (A + B)^T\n",
        "A_plus_B = A + B\n",
        "right_side = A_plus_B.t()\n",
        "\n",
        "# Verify equality\n",
        "is_equal = torch.equal(left_side, right_side)\n",
        "\n",
        "print(\"Tensor A:\")\n",
        "print(A)\n",
        "print(\"\\nTensor B:\")\n",
        "print(B)\n",
        "print(\"\\nA^T + B^T:\")\n",
        "print(left_side)\n",
        "print(\"\\n(A + B)^T:\")\n",
        "print(right_side)\n",
        "print(\"\\nIs A^T + B^T equal to (A + B)^T?\", is_equal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl7ihiWfdMBC",
        "outputId": "9c8b042e-4f69-48fc-bf23-b845abb24521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor A:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Tensor B:\n",
            "tensor([[ 7,  8,  9],\n",
            "        [10, 11, 12]])\n",
            "\n",
            "A^T + B^T:\n",
            "tensor([[ 8, 14],\n",
            "        [10, 16],\n",
            "        [12, 18]])\n",
            "\n",
            "(A + B)^T:\n",
            "tensor([[ 8, 14],\n",
            "        [10, 16],\n",
            "        [12, 18]])\n",
            "\n",
            "Is A^T + B^T equal to (A + B)^T? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Can you prove the result by using only the results of the previous two exercises?"
      ],
      "metadata": {
        "id": "-0sQIYwDe7yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample square matrix\n",
        "A = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6],\n",
        "                  [7, 8, 9]])\n",
        "\n",
        "# Compute A + A^T\n",
        "A_T = A.t()\n",
        "sum_A_AT = A + A_T\n",
        "\n",
        "# Check if A + A^T is symmetric by comparing with its transpose\n",
        "sum_A_AT_T = sum_A_AT.t()\n",
        "is_symmetric = torch.equal(sum_A_AT, sum_A_AT_T)\n",
        "\n",
        "print(\"Matrix A:\")\n",
        "print(A)\n",
        "print(\"\\nA + A^T:\")\n",
        "print(sum_A_AT)\n",
        "print(\"\\nTranspose of (A + A^T):\")\n",
        "print(sum_A_AT_T)\n",
        "print(\"\\nIs A + A^T symmetric?\", is_symmetric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiX8EGotefFZ",
        "outputId": "c550fc4e-40f9-40cb-e2f3-d77df81e2956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "\n",
            "A + A^T:\n",
            "tensor([[ 2,  6, 10],\n",
            "        [ 6, 10, 14],\n",
            "        [10, 14, 18]])\n",
            "\n",
            "Transpose of (A + A^T):\n",
            "tensor([[ 2,  6, 10],\n",
            "        [ 6, 10, 14],\n",
            "        [10, 14, 18]])\n",
            "\n",
            "Is A + A^T symmetric? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. We defined the tensor `X` of shape (2, 3, 4) in this section. What is the output of `len(X)`? Write your answer without implementing any code, then check your answer using code."
      ],
      "metadata": {
        "id": "LGr9VVCze0z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tensor X with shape (2, 3, 4)\n",
        "X = torch.randn(2, 3, 4)\n",
        "\n",
        "# Compute len(X)\n",
        "length = len(X)\n",
        "\n",
        "print(\"Tensor X shape:\", X.shape)\n",
        "print(\"len(X):\", length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTevxG9jew_T",
        "outputId": "d65639e6-8b54-4a96-92f4-5a87bde5f0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor X shape: torch.Size([2, 3, 4])\n",
            "len(X): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tensor X with shape (2, 3, 4)\n",
        "X = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
        "                  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]])\n",
        "\n",
        "# Compute len(X)\n",
        "length = len(X)\n",
        "\n",
        "print(\"Tensor X shape:\", X.shape)\n",
        "print(\"len(X):\", length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1muadMcf0wE",
        "outputId": "9dfe34aa-5413-4ab9-b25b-2e4e5ca495aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor X shape: torch.Size([2, 3, 4])\n",
            "len(X): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. For a tensor `X` of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of `X`? What is that axis?"
      ],
      "metadata": {
        "id": "sXk_PQv9g9_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensors with different shapes\n",
        "X1 = torch.randn(2, 3, 4)  # 3D tensor\n",
        "X2 = torch.randn(5)        # 1D tensor\n",
        "X3 = torch.randn(4, 2)     # 2D tensor\n",
        "X4 = torch.randn(3, 2, 5, 6)  # 4D tensor\n",
        "\n",
        "# Compute len() for each tensor and compare with size of first axis\n",
        "print(\"Tensor X1 shape:\", X1.shape, \"len(X1):\", len(X1), \"First axis size:\", X1.shape[0])\n",
        "print(\"Tensor X2 shape:\", X2.shape, \"len(X2):\", len(X2), \"First axis size:\", X2.shape[0])\n",
        "print(\"Tensor X3 shape:\", X3.shape, \"len(X3):\", len(X3), \"First axis size:\", X3.shape[0])\n",
        "print(\"Tensor X4 shape:\", X4.shape, \"len(X4):\", len(X4), \"First axis size:\", X4.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP_CpPrWg-ba",
        "outputId": "cab689a4-273c-451a-de0a-3b00c58d9959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor X1 shape: torch.Size([2, 3, 4]) len(X1): 2 First axis size: 2\n",
            "Tensor X2 shape: torch.Size([5]) len(X2): 5 First axis size: 5\n",
            "Tensor X3 shape: torch.Size([4, 2]) len(X3): 4 First axis size: 4\n",
            "Tensor X4 shape: torch.Size([3, 2, 5, 6]) len(X4): 3 First axis size: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When traveling between two points in downtown Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?"
      ],
      "metadata": {
        "id": "qm68k8VJilV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two points in Manhattan grid (avenue, street)\n",
        "point1 = torch.tensor([3, 5])  # (avenue, street)\n",
        "point2 = torch.tensor([7, 8])\n",
        "\n",
        "# Calculate Manhattan distance: |x2 - x1| + |y2 - y1|\n",
        "manhattan_distance = torch.sum(torch.abs(point2 - point1))\n",
        "\n",
        "# Output results\n",
        "print(\"Point 1 (avenue, street):\", point1)\n",
        "print(\"Point 2 (avenue, street):\", point2)\n",
        "print(\"Manhattan distance (avenues + streets):\", manhattan_distance.item())\n",
        "print(\"Diagonal travel possible? No, restricted to grid lines.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M72ZXDeihwn",
        "outputId": "d5bf95b4-47c6-40cc-e30a-787e03d87609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Point 1 (avenue, street): tensor([3, 5])\n",
            "Point 2 (avenue, street): tensor([7, 8])\n",
            "Manhattan distance (avenues + streets): 7\n",
            "Diagonal travel possible? No, restricted to grid lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs along axes 0, 1, and 2?"
      ],
      "metadata": {
        "id": "GcYzH1mjizEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with shape (2, 3, 4)\n",
        "X = torch.randn(2, 3, 4)\n",
        "\n",
        "# Compute sums along axes 0, 1, and 2\n",
        "sum_axis0 = X.sum(axis=0)\n",
        "sum_axis1 = X.sum(axis=1)\n",
        "sum_axis2 = X.sum(axis=2)\n",
        "\n",
        "# Print shapes\n",
        "print(\"Original tensor shape:\", X.shape)\n",
        "print(\"Shape of sum along axis 0:\", sum_axis0.shape)\n",
        "print(\"Shape of sum along axis 1:\", sum_axis1.shape)\n",
        "print(\"Shape of sum along axis 2:\", sum_axis2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eB-Goj7izg_",
        "outputId": "e065c448-962a-4e6b-b1ff-238dfb0d55a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor shape: torch.Size([2, 3, 4])\n",
            "Shape of sum along axis 0: torch.Size([3, 4])\n",
            "Shape of sum along axis 1: torch.Size([2, 4])\n",
            "Shape of sum along axis 2: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Feed a tensor with three or more axes to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape?"
      ],
      "metadata": {
        "id": "zFjCdyAMjSPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with shape (2, 3, 4)\n",
        "X = torch.randn(2, 3, 4)\n",
        "\n",
        "# Compute the Frobenius norm\n",
        "norm = torch.linalg.norm(X)\n",
        "\n",
        "# Compute norm along specific axes (e.g., axis=(1, 2)) for comparison\n",
        "norm_axes = torch.linalg.norm(X, dim=(1, 2))\n",
        "\n",
        "print(\"Tensor X shape:\", X.shape)\n",
        "print(\"Tensor X:\")\n",
        "print(X)\n",
        "print(\"Frobenius norm of X:\", norm.item())\n",
        "print(\"Norm along axes (1, 2) shape:\", norm_axes.shape)\n",
        "print(\"Norm along axes (1, 2):\", norm_axes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "105Bmon5jY9K",
        "outputId": "c88b194c-5afc-4a9a-a36e-9757a8724434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor X shape: torch.Size([2, 3, 4])\n",
            "Tensor X:\n",
            "tensor([[[-0.4086, -0.8933,  0.3617, -1.1050],\n",
            "         [-0.0902,  0.9664, -0.3586, -0.6783],\n",
            "         [-0.5909, -0.4524,  1.0552, -1.1534]],\n",
            "\n",
            "        [[-2.4534, -0.2908,  0.8685,  0.5183],\n",
            "         [-0.6063, -0.4720,  2.0130,  0.0468],\n",
            "         [-1.8592,  1.2989, -0.4654,  1.3795]]])\n",
            "Frobenius norm of X: 5.087274551391602\n",
            "Norm along axes (1, 2) shape: torch.Size([2])\n",
            "Norm along axes (1, 2): tensor([2.6164, 4.3629])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{14}}$, initialized with Gaussian random variables. You want to compute the product $\\mathbf{A} \\mathbf{B} \\mathbf{C}$. Is there any difference in memory footprint and speed, depending on whether you compute $(\\mathbf{A} \\mathbf{B}) \\mathbf{C}$ or $\\mathbf{A} (\\mathbf{B} \\mathbf{C})$. Why?"
      ],
      "metadata": {
        "id": "72gh3bGfklDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Define matrix dimensions\n",
        "n1, n2, n3, n4 = 2**10, 2**16, 2**5, 2**14\n",
        "\n",
        "# Create large matrices with Gaussian random variables\n",
        "A = torch.randn(n1, n2, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "B = torch.randn(n2, n3, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "C = torch.randn(n3, n4, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Function to measure memory and time\n",
        "def measure_operation(func, name):\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    start_time = time.time()\n",
        "    result = func()\n",
        "    end_time = time.time()\n",
        "    # Estimate memory usage of intermediate and final results\n",
        "    memory_bytes = result.element_size() * result.numel()\n",
        "    if torch.cuda.is_available():\n",
        "        peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "    else:\n",
        "        peak_memory = \"N/A (CPU)\"\n",
        "    print(f\"{name} - Time: {end_time - start_time:.4f} seconds, Output shape: {result.shape}, \"\n",
        "          f\"Output memory: {memory_bytes / 1024**2:.2f} MB, Peak memory: {peak_memory} MB\")\n",
        "    return result\n",
        "\n",
        "# Compute (A B) C\n",
        "def compute_ABC():\n",
        "    AB = torch.matmul(A, B)\n",
        "    return torch.matmul(AB, C)\n",
        "\n",
        "# Compute A (B C)\n",
        "def compute_A_BC():\n",
        "    BC = torch.matmul(B, C)\n",
        "    return torch.matmul(A, BC)\n",
        "\n",
        "# Run and measure both computations\n",
        "print(\"Computing (A B) C:\")\n",
        "result1 = measure_operation(compute_ABC, \"(A B) C\")\n",
        "print(\"\\nComputing A (B C):\")\n",
        "result2 = measure_operation(compute_A_BC, \"A (B C)\")\n",
        "\n",
        "# Verify results are equivalent\n",
        "is_equal = torch.allclose(result1, result2, atol=1e-5)\n",
        "print(\"\\nAre results equal?\", is_equal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBgtD4pYkmJz",
        "outputId": "fa4cb887-64b8-4547-b091-10e26dea62c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing (A B) C:\n",
            "(A B) C - Time: 0.1379 seconds, Output shape: torch.Size([1024, 16384]), Output memory: 64.00 MB, Peak memory: N/A (CPU) MB\n",
            "\n",
            "Computing A (B C):\n",
            "A (B C) - Time: 36.7092 seconds, Output shape: torch.Size([1024, 16384]), Output memory: 64.00 MB, Peak memory: N/A (CPU) MB\n",
            "\n",
            "Are results equal? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}}$. Is there any difference in speed depending on whether you compute $\\mathbf{A} \\mathbf{B}$ or $\\mathbf{A} \\mathbf{C}^\\top$? Why? What changes if you initialize $\\mathbf{C} = \\mathbf{B}^\\top$ without cloning memory? Why?"
      ],
      "metadata": {
        "id": "XdAs7HpzmhI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define matrix dimensions\n",
        "n1, n2, n3 = 2**10, 2**16, 2**5  # 1024, 65536, 32\n",
        "\n",
        "# Create matrices with Gaussian random variables\n",
        "A = torch.randn(n1, n2, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "B = torch.randn(n2, n3, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "C = torch.randn(n3, n2, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Function to measure time and memory\n",
        "def measure_operation(func, name):\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    start_time = time.time()\n",
        "    result = func()\n",
        "    end_time = time.time()\n",
        "    memory_bytes = result.element_size() * result.numel()\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2 if torch.cuda.is_available() else \"N/A (CPU)\"\n",
        "    print(f\"{name} - Time: {end_time - start_time:.4f} seconds, Output shape: {result.shape}, \"\n",
        "          f\"Output memory: {memory_bytes / 1024**2:.2f} MB, Peak memory: {peak_memory} MB\")\n",
        "    return result\n",
        "\n",
        "# Compute A B\n",
        "def compute_AB():\n",
        "    print(f\"A shape: {A.shape}, B shape: {B.shape}\")\n",
        "    return torch.matmul(A, B)\n",
        "\n",
        "# Compute A C^T\n",
        "def compute_A_CT():\n",
        "    print(f\"A shape: {A.shape}, C^T shape: {C.t().shape}\")\n",
        "    return torch.matmul(A, C.t())\n",
        "\n",
        "# Compute A C^T with C = B^T (no cloning)\n",
        "def compute_A_BT():\n",
        "    C_BT = B.t()  # C = B^T, no cloning\n",
        "    print(f\"A shape: {A.shape}, C (=B^T) shape: {C_BT.shape}, C^T shape: {C_BT.t().shape}\")\n",
        "    return torch.matmul(A, C_BT.t())  # Use C^T = (B^T)^T = B\n",
        "\n",
        "# Run and measure computations\n",
        "print(\"Computing A B:\")\n",
        "result_AB = measure_operation(compute_AB, \"A B\")\n",
        "print(\"\\nComputing A C^T:\")\n",
        "result_A_CT = measure_operation(compute_A_CT, \"A C^T\")\n",
        "print(\"\\nComputing A C^T with C = B^T (no cloning):\")\n",
        "result_A_BT = measure_operation(compute_A_BT, \"A (C^T) where C = B^T\")\n",
        "\n",
        "# Verify results when C = B^T\n",
        "C = B.t()  # Set C as B^T without cloning\n",
        "is_equal = torch.allclose(result_AB, result_A_BT, atol=1e-5)\n",
        "print(\"\\nIs A B equal to A (C^T) when C = B^T?\", is_equal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT1AL1F6GB3v",
        "outputId": "72381483-bd7b-43f9-b586-9c1ca1e47e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing A B:\n",
            "A shape: torch.Size([1024, 65536]), B shape: torch.Size([65536, 32])\n",
            "A B - Time: 0.0915 seconds, Output shape: torch.Size([1024, 32]), Output memory: 0.12 MB, Peak memory: N/A (CPU) MB\n",
            "\n",
            "Computing A C^T:\n",
            "A shape: torch.Size([1024, 65536]), C^T shape: torch.Size([65536, 32])\n",
            "A C^T - Time: 0.1032 seconds, Output shape: torch.Size([1024, 32]), Output memory: 0.12 MB, Peak memory: N/A (CPU) MB\n",
            "\n",
            "Computing A C^T with C = B^T (no cloning):\n",
            "A shape: torch.Size([1024, 65536]), C (=B^T) shape: torch.Size([32, 65536]), C^T shape: torch.Size([65536, 32])\n",
            "A (C^T) where C = B^T - Time: 0.0924 seconds, Output shape: torch.Size([1024, 32]), Output memory: 0.12 MB, Peak memory: N/A (CPU) MB\n",
            "\n",
            "Is A B equal to A (C^T) when C = B^T? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Consider three matrices, say $\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{100 \\times 200}$. Construct a tensor with three axes by stacking $[\\mathbf{A}, \\mathbf{B}, \\mathbf{C}]$. What is the dimensionality? Slice out the second coordinate of the third axis to recover $\\mathbf{B}$. Check that your answer is correct."
      ],
      "metadata": {
        "id": "q_CzYDVpImpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create three matrices A, B, C with shape (100, 200)\n",
        "A = torch.randn(100, 200)\n",
        "B = torch.randn(100, 200)\n",
        "C = torch.randn(100, 200)\n",
        "\n",
        "# Stack matrices to create a tensor with shape (3, 100, 200)\n",
        "tensor = torch.stack([A, B, C], dim=0)\n",
        "\n",
        "# Get the dimensionality of the tensor\n",
        "tensor_shape = tensor.shape\n",
        "\n",
        "# Slice out the second coordinate of the first axis (index 1) to recover B\n",
        "B_recovered = tensor[1]\n",
        "\n",
        "# Check if B_recovered matches B\n",
        "is_equal = torch.equal(B, B_recovered)\n",
        "\n",
        "# Print results\n",
        "print(\"Tensor shape:\", tensor_shape)\n",
        "print(\"Shape of B_recovered:\", B_recovered.shape)\n",
        "print(\"Is B_recovered equal to B?\", is_equal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUYItgXOIj7e",
        "outputId": "dd9f0e6b-31f4-49a1-f313-028736430cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor shape: torch.Size([3, 100, 200])\n",
            "Shape of B_recovered: torch.Size([100, 200])\n",
            "Is B_recovered equal to B? True\n"
          ]
        }
      ]
    }
  ]
}